{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d57223a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "PATH = \"/Users/beratzengin/Desktop/Github/EcoAir SmartCity Predictor/Feature Engineering\"\n",
    "train_df = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(PATH, 'test.csv'))\n",
    "\n",
    "target = 'PM10'\n",
    "features = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target in features: features.remove(target)\n",
    "\n",
    "X_train = train_df[features].fillna(0).values\n",
    "y_train = train_df[target].fillna(train_df[target].mean()).values\n",
    "X_test = test_df[features].fillna(0).values\n",
    "y_test = test_df[target].fillna(test_df[target].mean()).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b365b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beratzengin/Desktop/Github/EcoAir SmartCity Predictor/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.2), # Pass Overfitting \n",
    "    \n",
    "    # Hidden Layers\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    # Output Layer\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a450ed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 294.1500 - mae: 8.9028 - val_loss: 237.4339 - val_mae: 7.8508\n",
      "Epoch 2/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 378us/step - loss: 229.5005 - mae: 7.8596 - val_loss: 222.3568 - val_mae: 7.9928\n",
      "Epoch 3/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 224.4837 - mae: 7.7312 - val_loss: 205.7773 - val_mae: 7.1368\n",
      "Epoch 4/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 223.2422 - mae: 7.6605 - val_loss: 216.1060 - val_mae: 7.3961\n",
      "Epoch 5/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 386us/step - loss: 219.7162 - mae: 7.6250 - val_loss: 210.2262 - val_mae: 7.5009\n",
      "Epoch 6/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 219.0588 - mae: 7.5870 - val_loss: 206.8537 - val_mae: 7.4772\n",
      "Epoch 7/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 218.1691 - mae: 7.5586 - val_loss: 201.5382 - val_mae: 7.2775\n",
      "Epoch 8/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 215.0052 - mae: 7.5193 - val_loss: 202.7430 - val_mae: 7.1827\n",
      "Epoch 9/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 214.3208 - mae: 7.5107 - val_loss: 205.0039 - val_mae: 7.2222\n",
      "Epoch 10/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 215.0587 - mae: 7.5208 - val_loss: 202.8827 - val_mae: 7.1836\n",
      "Epoch 11/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 212.7443 - mae: 7.4718 - val_loss: 210.1450 - val_mae: 7.3250\n",
      "Epoch 12/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 211.5879 - mae: 7.4689 - val_loss: 199.0639 - val_mae: 7.1413\n",
      "Epoch 13/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 211.7883 - mae: 7.4469 - val_loss: 197.8565 - val_mae: 7.1296\n",
      "Epoch 14/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 211.1568 - mae: 7.4360 - val_loss: 197.8456 - val_mae: 7.3168\n",
      "Epoch 15/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 210.4301 - mae: 7.4247 - val_loss: 201.8195 - val_mae: 7.1291\n",
      "Epoch 16/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 209.0825 - mae: 7.4008 - val_loss: 198.0470 - val_mae: 7.2787\n",
      "Epoch 17/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 209.2746 - mae: 7.3878 - val_loss: 197.0947 - val_mae: 7.2204\n",
      "Epoch 18/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 388us/step - loss: 206.6666 - mae: 7.3659 - val_loss: 199.6817 - val_mae: 7.1825\n",
      "Epoch 19/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 207.9106 - mae: 7.3881 - val_loss: 206.6616 - val_mae: 7.3179\n",
      "Epoch 20/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 206.4115 - mae: 7.3465 - val_loss: 197.4393 - val_mae: 7.1808\n",
      "Epoch 21/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 206.2187 - mae: 7.3563 - val_loss: 196.2789 - val_mae: 7.2929\n",
      "Epoch 22/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 417us/step - loss: 204.5686 - mae: 7.3117 - val_loss: 198.9320 - val_mae: 7.5602\n",
      "Epoch 23/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step - loss: 204.1236 - mae: 7.3170 - val_loss: 197.5702 - val_mae: 7.2340\n",
      "Epoch 24/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 205.1584 - mae: 7.3178 - val_loss: 201.4025 - val_mae: 7.6629\n",
      "Epoch 25/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 205.4496 - mae: 7.3184 - val_loss: 196.6390 - val_mae: 7.3990\n",
      "Epoch 26/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 203.4545 - mae: 7.3044 - val_loss: 202.4088 - val_mae: 7.6770\n",
      "Epoch 27/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 202.5979 - mae: 7.3022 - val_loss: 199.6439 - val_mae: 7.7158\n",
      "Epoch 28/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 204.2549 - mae: 7.2886 - val_loss: 200.2870 - val_mae: 7.6816\n",
      "Epoch 29/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 203.0797 - mae: 7.2809 - val_loss: 196.5402 - val_mae: 7.5411\n",
      "Epoch 30/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 202.1025 - mae: 7.2661 - val_loss: 201.3106 - val_mae: 7.7574\n",
      "Epoch 31/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 416us/step - loss: 202.1954 - mae: 7.2557 - val_loss: 210.4920 - val_mae: 7.9119\n",
      "Epoch 32/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 200.6458 - mae: 7.2688 - val_loss: 199.6240 - val_mae: 7.6963\n",
      "Epoch 33/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 202.0948 - mae: 7.2567 - val_loss: 199.6895 - val_mae: 7.7697\n",
      "Epoch 34/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 200.4600 - mae: 7.2557 - val_loss: 199.7052 - val_mae: 7.7155\n",
      "Epoch 35/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 201.7122 - mae: 7.2623 - val_loss: 204.3340 - val_mae: 8.0528\n",
      "Epoch 36/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 201.8693 - mae: 7.2657 - val_loss: 205.2042 - val_mae: 8.3270\n",
      "Epoch 37/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 198.8947 - mae: 7.2296 - val_loss: 202.1174 - val_mae: 7.9839\n",
      "Epoch 38/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 199.3698 - mae: 7.2475 - val_loss: 204.7147 - val_mae: 8.0771\n",
      "Epoch 39/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 198.4071 - mae: 7.2352 - val_loss: 203.8601 - val_mae: 8.2126\n",
      "Epoch 40/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 197.4604 - mae: 7.2131 - val_loss: 202.4653 - val_mae: 8.0712\n",
      "Epoch 41/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 198.7393 - mae: 7.2328 - val_loss: 212.2787 - val_mae: 8.6522\n",
      "Epoch 42/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 196.0943 - mae: 7.2138 - val_loss: 207.6676 - val_mae: 8.2320\n",
      "Epoch 43/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 198.6914 - mae: 7.2246 - val_loss: 211.6837 - val_mae: 8.4918\n",
      "Epoch 44/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 195.8256 - mae: 7.1964 - val_loss: 223.7621 - val_mae: 9.4512\n",
      "Epoch 45/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 195.3595 - mae: 7.1810 - val_loss: 210.1772 - val_mae: 8.4030\n",
      "Epoch 46/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 196.4369 - mae: 7.2002 - val_loss: 204.4343 - val_mae: 8.3464\n",
      "Epoch 47/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 195.7947 - mae: 7.1874 - val_loss: 214.2301 - val_mae: 8.7682\n",
      "Epoch 48/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 194.8211 - mae: 7.1796 - val_loss: 211.2305 - val_mae: 8.7941\n",
      "Epoch 49/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 194.8607 - mae: 7.1894 - val_loss: 211.8323 - val_mae: 8.6395\n",
      "Epoch 50/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 194.9263 - mae: 7.1837 - val_loss: 206.9134 - val_mae: 8.3048\n",
      "Epoch 51/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 194.6141 - mae: 7.1861 - val_loss: 205.8041 - val_mae: 8.4171\n",
      "Epoch 52/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 193.6139 - mae: 7.1800 - val_loss: 199.4483 - val_mae: 7.9363\n",
      "Epoch 53/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 193.9096 - mae: 7.1575 - val_loss: 200.0299 - val_mae: 8.0786\n",
      "Epoch 54/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 192.3584 - mae: 7.1482 - val_loss: 218.4506 - val_mae: 8.8799\n",
      "Epoch 55/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 192.9759 - mae: 7.1671 - val_loss: 205.0656 - val_mae: 8.4089\n",
      "Epoch 56/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 406us/step - loss: 193.2145 - mae: 7.1711 - val_loss: 205.2332 - val_mae: 8.3250\n",
      "Epoch 57/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 193.7035 - mae: 7.1661 - val_loss: 216.2553 - val_mae: 9.0960\n",
      "Epoch 58/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 192.0535 - mae: 7.1570 - val_loss: 202.2374 - val_mae: 8.3317\n",
      "Epoch 59/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 191.6612 - mae: 7.1600 - val_loss: 204.9896 - val_mae: 8.4665\n",
      "Epoch 60/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 192.2632 - mae: 7.1479 - val_loss: 214.6228 - val_mae: 8.8009\n",
      "Epoch 61/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 190.3261 - mae: 7.1368 - val_loss: 215.4907 - val_mae: 9.0515\n",
      "Epoch 62/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 190.1304 - mae: 7.1465 - val_loss: 208.4792 - val_mae: 8.7471\n",
      "Epoch 63/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 190.5788 - mae: 7.1415 - val_loss: 215.6342 - val_mae: 9.1349\n",
      "Epoch 64/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 190.4225 - mae: 7.1410 - val_loss: 206.7233 - val_mae: 8.5875\n",
      "Epoch 65/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 190.4244 - mae: 7.1314 - val_loss: 225.2449 - val_mae: 9.4283\n",
      "Epoch 66/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 190.0372 - mae: 7.1352 - val_loss: 215.5464 - val_mae: 9.0924\n",
      "Epoch 67/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 190.4896 - mae: 7.1321 - val_loss: 215.4969 - val_mae: 8.5774\n",
      "Epoch 68/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 189.5507 - mae: 7.1290 - val_loss: 212.6751 - val_mae: 9.0193\n",
      "Epoch 69/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 394us/step - loss: 188.7008 - mae: 7.1251 - val_loss: 214.3526 - val_mae: 8.9907\n",
      "Epoch 70/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 189.3212 - mae: 7.1338 - val_loss: 206.5381 - val_mae: 8.6225\n",
      "Epoch 71/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 188.9431 - mae: 7.1185 - val_loss: 201.3568 - val_mae: 8.3128\n",
      "Epoch 72/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 188.1080 - mae: 7.1175 - val_loss: 214.3970 - val_mae: 8.8225\n",
      "Epoch 73/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 401us/step - loss: 188.9263 - mae: 7.1160 - val_loss: 209.5456 - val_mae: 8.7462\n",
      "Epoch 74/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 187.8338 - mae: 7.1335 - val_loss: 212.0477 - val_mae: 8.7712\n",
      "Epoch 75/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 188.1844 - mae: 7.1237 - val_loss: 214.6562 - val_mae: 8.7546\n",
      "Epoch 76/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 186.8085 - mae: 7.1112 - val_loss: 212.9873 - val_mae: 8.7243\n",
      "Epoch 77/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 187.9273 - mae: 7.1184 - val_loss: 210.6930 - val_mae: 8.8766\n",
      "Epoch 78/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 403us/step - loss: 186.3036 - mae: 7.1105 - val_loss: 214.7561 - val_mae: 8.7479\n",
      "Epoch 79/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 393us/step - loss: 185.7300 - mae: 7.0866 - val_loss: 206.4400 - val_mae: 8.4210\n",
      "Epoch 80/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 187.0302 - mae: 7.1076 - val_loss: 211.3155 - val_mae: 8.9023\n",
      "Epoch 81/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 404us/step - loss: 186.3039 - mae: 7.1037 - val_loss: 210.0144 - val_mae: 8.8456\n",
      "Epoch 82/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 186.4747 - mae: 7.1081 - val_loss: 222.2315 - val_mae: 9.1164\n",
      "Epoch 83/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 395us/step - loss: 186.6886 - mae: 7.1055 - val_loss: 221.0819 - val_mae: 9.4160\n",
      "Epoch 84/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 186.7535 - mae: 7.0998 - val_loss: 212.3961 - val_mae: 8.8748\n",
      "Epoch 85/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 408us/step - loss: 185.9317 - mae: 7.0995 - val_loss: 217.1200 - val_mae: 9.2424\n",
      "Epoch 86/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 186.1189 - mae: 7.1129 - val_loss: 213.3709 - val_mae: 8.8093\n",
      "Epoch 87/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 410us/step - loss: 189.2239 - mae: 7.0956 - val_loss: 215.6449 - val_mae: 8.6640\n",
      "Epoch 88/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 185.6269 - mae: 7.0988 - val_loss: 213.4519 - val_mae: 8.9867\n",
      "Epoch 89/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 400us/step - loss: 184.9445 - mae: 7.0916 - val_loss: 222.8941 - val_mae: 9.0968\n",
      "Epoch 90/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 392us/step - loss: 183.2609 - mae: 7.0759 - val_loss: 211.6871 - val_mae: 8.8034\n",
      "Epoch 91/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 183.7823 - mae: 7.0681 - val_loss: 219.7457 - val_mae: 8.7113\n",
      "Epoch 92/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 183.6090 - mae: 7.0611 - val_loss: 220.3048 - val_mae: 9.3918\n",
      "Epoch 93/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 389us/step - loss: 183.5192 - mae: 7.0849 - val_loss: 214.6062 - val_mae: 8.9537\n",
      "Epoch 94/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step - loss: 182.7227 - mae: 7.0671 - val_loss: 218.2830 - val_mae: 9.2495\n",
      "Epoch 95/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 391us/step - loss: 185.2476 - mae: 7.0792 - val_loss: 219.7202 - val_mae: 9.3317\n",
      "Epoch 96/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 397us/step - loss: 183.1907 - mae: 7.0606 - val_loss: 218.7566 - val_mae: 9.1561\n",
      "Epoch 97/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 390us/step - loss: 183.9484 - mae: 7.0669 - val_loss: 207.8573 - val_mae: 8.4512\n",
      "Epoch 98/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 399us/step - loss: 183.0640 - mae: 7.0655 - val_loss: 218.9896 - val_mae: 9.0347\n",
      "Epoch 99/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 387us/step - loss: 181.1831 - mae: 7.0564 - val_loss: 218.6061 - val_mae: 9.1464\n",
      "Epoch 100/100\n",
      "\u001b[1m4170/4170\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 396us/step - loss: 181.5543 - mae: 7.0521 - val_loss: 214.8153 - val_mae: 8.7014\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2, \n",
    "    epochs=100,           \n",
    "    batch_size=32,        \n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "041241eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If u reading this u can 100% believe me, me to sometimes don't get these things...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "573d6a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beratzengin/Desktop/Github/EcoAir SmartCity Predictor/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:106: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 408us/step - loss: 425.8317 - mae: 10.9210 - val_loss: 212.5635 - val_mae: 7.3616\n",
      "Epoch 2/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - loss: 249.9511 - mae: 8.1761 - val_loss: 210.7646 - val_mae: 7.3069\n",
      "Epoch 3/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 393us/step - loss: 236.9920 - mae: 7.9848 - val_loss: 209.9723 - val_mae: 7.3703\n",
      "Epoch 4/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 378us/step - loss: 233.3610 - mae: 7.8739 - val_loss: 210.8461 - val_mae: 7.4547\n",
      "Epoch 5/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381us/step - loss: 233.5837 - mae: 7.8166 - val_loss: 209.4151 - val_mae: 7.2680\n",
      "Epoch 6/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - loss: 226.4444 - mae: 7.7248 - val_loss: 230.6264 - val_mae: 7.7801\n",
      "Epoch 7/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 381us/step - loss: 227.5439 - mae: 7.7327 - val_loss: 209.8120 - val_mae: 7.4274\n",
      "Epoch 8/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 398us/step - loss: 225.7940 - mae: 7.7089 - val_loss: 206.3420 - val_mae: 7.2629\n",
      "Epoch 9/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 226.8417 - mae: 7.6843 - val_loss: 208.2100 - val_mae: 7.5281\n",
      "Epoch 10/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 224.8099 - mae: 7.6506 - val_loss: 207.1914 - val_mae: 7.2789\n",
      "Epoch 11/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 223.6002 - mae: 7.6219 - val_loss: 214.2993 - val_mae: 7.5460\n",
      "Epoch 12/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 418us/step - loss: 221.6450 - mae: 7.6027 - val_loss: 205.8409 - val_mae: 7.3910\n",
      "Epoch 13/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 388us/step - loss: 224.3356 - mae: 7.6025 - val_loss: 209.3039 - val_mae: 7.7601\n",
      "Epoch 14/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - loss: 222.1359 - mae: 7.5610 - val_loss: 208.5176 - val_mae: 7.6723\n",
      "Epoch 15/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 221.1023 - mae: 7.5333 - val_loss: 216.6048 - val_mae: 8.1507\n",
      "Epoch 16/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 218.5782 - mae: 7.5017 - val_loss: 211.9633 - val_mae: 7.9968\n",
      "Epoch 17/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 220.7608 - mae: 7.5060 - val_loss: 219.4562 - val_mae: 8.6552\n",
      "Epoch 18/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 384us/step - loss: 217.7907 - mae: 7.4699 - val_loss: 225.4034 - val_mae: 8.8542\n",
      "Epoch 19/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - loss: 219.9473 - mae: 7.4883 - val_loss: 214.6414 - val_mae: 8.2080\n",
      "Epoch 20/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 401us/step - loss: 218.6996 - mae: 7.4707 - val_loss: 219.2651 - val_mae: 8.6254\n",
      "Epoch 21/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 383us/step - loss: 217.0755 - mae: 7.4773 - val_loss: 224.8695 - val_mae: 8.9379\n",
      "Epoch 22/150\n",
      "\u001b[1m2085/2085\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - loss: 218.1107 - mae: 7.4628 - val_loss: 221.8093 - val_mae: 8.6477\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3), \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    restore_best_weights=True \n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=150, \n",
    "    batch_size=64, \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47c6d429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1303/1303\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174us/step\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "log_path = os.path.join(PATH, 'model_performances.csv')\n",
    "log_data = [\n",
    "    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    '009',\n",
    "    'Deep Learning (ANN)',\n",
    "    mae,\n",
    "    rmse,\n",
    "    r2,\n",
    "    f'all_numeric_{X_train_scaled.shape[1]}_cols',\n",
    "    '3-layer MLP with EarlyStopping and Dropout'\n",
    "]\n",
    "\n",
    "with open(log_path, 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(log_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf4791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
